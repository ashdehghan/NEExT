{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from NEExT import NEExT\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_file_path = \"../data/ABCDO-full/edges.csv\"\n",
    "node_graph_mapping_file_path = \"../data/ABCDO-full/graph_mapping.csv\"\n",
    "features_file_path = \"../data/ABCDO-full/features.csv\"\n",
    "\n",
    "edges = pd.read_csv(edge_file_path)\n",
    "mapping = pd.read_csv(node_graph_mapping_file_path)\n",
    "_features = pd.read_csv(features_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def semi_supervised_set(df, col=\"is_outlier\", hide_frac={0: 0.0, 1: 0.0}, seed=42):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    for _cls, frac in hide_frac.items():\n",
    "        mask = df[col] == _cls  \n",
    "        drop_indices = np.random.choice(df[mask].index, size=int(len(df[mask]) * frac), replace=False)\n",
    "        df.loc[drop_indices, col] = -1\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "_features = semi_supervised_set(_features)\n",
    "\n",
    "edge_file_path = \"../data/ABCDO-partial/edges.csv\"\n",
    "node_graph_mapping_file_path = \"../data/ABCDO-partial/graph_mapping.csv\"\n",
    "features_file_path = \"../data/ABCDO-partial/features.csv\"\n",
    "\n",
    "os.makedirs('../data/ABCDO-partial/', exist_ok=True)\n",
    "edges.to_csv(edge_file_path)\n",
    "mapping.to_csv(node_graph_mapping_file_path)\n",
    "_features.to_csv(features_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NEExT.collections import EgonetCollection\n",
    "from NEExT.features import NodeFeatures, StructuralNodeFeatures\n",
    "from NEExT.io import GraphIO\n",
    "from NEExT.ml_models import MLModels\n",
    "\n",
    "target = \"is_outlier\"\n",
    "sample_size = 5\n",
    "\n",
    "graph_io = GraphIO()\n",
    "subgraph_collection = EgonetCollection()\n",
    "\n",
    "\n",
    "graph_collection = graph_io.read_from_csv(\n",
    "    edges_path=edge_file_path,\n",
    "    node_graph_mapping_path=node_graph_mapping_file_path,\n",
    "    node_features_path=features_file_path,\n",
    "    graph_type=\"igraph\",\n",
    ")\n",
    "subgraph_collection.create_egonets_from_graphs(\n",
    "    graph_collection=graph_collection,\n",
    "    egonet_target=target,\n",
    "    egonet_algorithm=\"k_hop_egonet\",\n",
    "    skip_features=[\"community_id\"],\n",
    "    max_hop_length=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1b89ca636146d88a03979516d26bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing structural node features:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "structural_node_features = StructuralNodeFeatures(\n",
    "    graph_collection=subgraph_collection,\n",
    "    feature_list=[\"all\"],\n",
    "    feature_vector_length=3,\n",
    "    n_jobs=8,\n",
    ")\n",
    "node_features = NodeFeatures(\n",
    "    subgraph_collection,\n",
    "    feature_list=[\"random_community_feature\"],\n",
    ")\n",
    "\n",
    "structural_features = structural_node_features.compute()\n",
    "features = node_features.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NEExT.builders import EmbeddingBuilder\n",
    "\n",
    "for s in [\"structural_embedding\"]:\n",
    "    emb_builder = EmbeddingBuilder(subgraph_collection, strategy=s, structural_features=structural_features, features=features,)\n",
    "    embeddings = emb_builder.compute(10, 6)\n",
    "    \n",
    "    ml_models = MLModels(\n",
    "        graph_collection=subgraph_collection,\n",
    "        embedding=embeddings,   \n",
    "        model_type='classifier',\n",
    "    )\n",
    "\n",
    "    # results = ml_models.compute()\n",
    "    # print(s, f\"Model trained with average accuracy: {np.mean(results['accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quak/miniconda3/envs/neext/lib/python3.11/site-packages/sklearn/semi_supervised/_self_training.py:288: UserWarning: y contains no unlabeled samples\n",
      "  warnings.warn(\"y contains no unlabeled samples\", UserWarning)\n",
      "/home/quak/miniconda3/envs/neext/lib/python3.11/site-packages/sklearn/semi_supervised/_self_training.py:288: UserWarning: y contains no unlabeled samples\n",
      "  warnings.warn(\"y contains no unlabeled samples\", UserWarning)\n",
      "/home/quak/miniconda3/envs/neext/lib/python3.11/site-packages/sklearn/semi_supervised/_self_training.py:288: UserWarning: y contains no unlabeled samples\n",
      "  warnings.warn(\"y contains no unlabeled samples\", UserWarning)\n",
      "/home/quak/miniconda3/envs/neext/lib/python3.11/site-packages/sklearn/semi_supervised/_self_training.py:288: UserWarning: y contains no unlabeled samples\n",
      "  warnings.warn(\"y contains no unlabeled samples\", UserWarning)\n",
      "/home/quak/miniconda3/envs/neext/lib/python3.11/site-packages/sklearn/semi_supervised/_self_training.py:288: UserWarning: y contains no unlabeled samples\n",
      "  warnings.warn(\"y contains no unlabeled samples\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "from typing import Any, Dict, List, Literal, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error, mean_squared_error, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from NEExT.collections import GraphCollection\n",
    "from NEExT.embeddings import Embeddings\n",
    "\n",
    "# Try to import XGBoost, but provide fallbacks if it fails\n",
    "try:\n",
    "    from xgboost import XGBClassifier, XGBRegressor\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "\n",
    "# Try to import SMOTE, but handle if it's not available\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    SMOTE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SMOTE_AVAILABLE = False\n",
    "\n",
    "# Import sklearn models as fallbacks\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "\n",
    "\n",
    "def _train_classifier_iteration(self, iteration_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Train and evaluate a classifier for a single iteration.\"\"\"\n",
    "        i = iteration_data['i']\n",
    "        X = iteration_data['X']\n",
    "        y = iteration_data['y']\n",
    "        feature_cols = iteration_data['feature_cols']\n",
    "        test_size = iteration_data['test_size']\n",
    "        random_state = iteration_data['random_state'] + i\n",
    "        balance_dataset = iteration_data['balance_dataset']\n",
    "        num_classes = iteration_data['num_classes']\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X[feature_cols], y, test_size=test_size, random_state=random_state,  shuffle=True,\n",
    "        )\n",
    "        \n",
    "        # # Balance dataset if requested\n",
    "        # if balance_dataset:\n",
    "        #     from imblearn.over_sampling import SMOTE\n",
    "        #     smote = SMOTE(random_state=random_state)\n",
    "        #     X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "        # Train model based on selected algorithm\n",
    "        if self.config.model_name == \"xgboost\" and XGBOOST_AVAILABLE:\n",
    "            model = XGBClassifier(random_state=random_state, n_jobs=1)\n",
    "        else:\n",
    "            model = RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                random_state=random_state,\n",
    "                n_jobs=1\n",
    "            )\n",
    "        from sklearn.semi_supervised import SelfTrainingClassifier\n",
    "        model = SelfTrainingClassifier(model, max_iter=10)\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Get feature importance if requested\n",
    "        feature_importance = None\n",
    "        if self.config.compute_feature_importance:\n",
    "            if self.config.model_name == \"xgboost\" and XGBOOST_AVAILABLE:\n",
    "                importance = model.feature_importances_\n",
    "            else:\n",
    "                importance = model.feature_importances_\n",
    "            \n",
    "            # Create feature importance ranking\n",
    "            importance_dict = dict(zip(feature_cols, importance))\n",
    "            sorted_features = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "            feature_importance = {feat: rank for rank, (feat, _) in enumerate(sorted_features, 1)}\n",
    "        \n",
    "        # Return results\n",
    "        return {\n",
    "            'model': model,\n",
    "            # these scores are useless here\n",
    "            # 'accuracy': accuracy_score(y_test, y_pred),\n",
    "            # 'recall': recall_score(y_test, y_pred, average='macro'),\n",
    "            # 'precision': precision_score(y_test, y_pred, average='macro'),\n",
    "            # 'f1_score': f1_score(y_test, y_pred, average='macro'),\n",
    "            'feature_importance': feature_importance\n",
    "        }\n",
    "\n",
    "self = ml_models\n",
    "feature_cols = [col for col in self.data_df.columns \n",
    "                       if col not in [\"graph_id\", \"label\", \"encoded_label\"]]\n",
    "\n",
    "iteration_data = [\n",
    "            {\n",
    "                'i': i,\n",
    "                'X': self.data_df,\n",
    "                'y': self.data_df[\"encoded_label\"],\n",
    "                'feature_cols': feature_cols,\n",
    "                'test_size': self.config.test_size,\n",
    "                'random_state': self.config.random_state,\n",
    "                'balance_dataset': self.config.balance_dataset,\n",
    "                'num_classes': self.num_classes\n",
    "            }\n",
    "            for i in range(self.config.sample_size)\n",
    "        ]\n",
    "executor_class = ProcessPoolExecutor if self.config.parallel_backend == \"process\" else ThreadPoolExecutor\n",
    "results = []\n",
    "\n",
    "results = [_train_classifier_iteration(self, iteration) for iteration in iteration_data]\n",
    "        \n",
    "        \n",
    "# Collect results\n",
    "models = [r['model'] for r in results]\n",
    "# accuracy_scores = [r['accuracy'] for r in results]\n",
    "# recall_scores = [r['recall'] for r in results]\n",
    "# precision_scores = [r['precision'] for r in results]\n",
    "# f1_scores = [r['f1_score'] for r in results]\n",
    "\n",
    "# Process feature importance if computed\n",
    "feature_importance_df = None\n",
    "if self.config.compute_feature_importance:\n",
    "    # Collect all rankings\n",
    "    all_rankings = []\n",
    "    for r in results:\n",
    "        all_rankings.append(r['feature_importance'])\n",
    "    \n",
    "    # Calculate average rank for each feature\n",
    "    feature_ranks = {}\n",
    "    for feature in feature_cols:\n",
    "        ranks = [r[feature] for r in all_rankings]\n",
    "        feature_ranks[feature] = np.mean(ranks)\n",
    "    \n",
    "    # Create sorted DataFrame\n",
    "    feature_importance_df = pd.DataFrame({'average_rank': feature_ranks}).sort_values('average_rank')\n",
    "\n",
    "# Return results with means and standard deviations\n",
    "out= {\n",
    "    \"model_type\": \"classifier\",\n",
    "    # \"accuracy\": accuracy_scores,\n",
    "    # \"accuracy_mean\": np.mean(accuracy_scores),\n",
    "    # \"accuracy_std\": np.std(accuracy_scores),\n",
    "    # \"recall\": recall_scores,\n",
    "    # \"recall_mean\": np.mean(recall_scores),\n",
    "    # \"recall_std\": np.std(recall_scores),\n",
    "    # \"precision\": precision_scores,\n",
    "    # \"precision_mean\": np.mean(precision_scores),\n",
    "    # \"precision_std\": np.std(precision_scores),\n",
    "    # \"f1_score\": f1_scores,\n",
    "    # \"f1_score_mean\": np.mean(f1_scores),\n",
    "    # \"f1_score_std\": np.std(f1_scores),\n",
    "    \"model\": models[-1],\n",
    "    \"classes\": [0, 1],\n",
    "    \"feature_columns\": feature_cols,\n",
    "    \"feature_importance\": feature_importance_df\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_type': 'classifier',\n",
       " 'model': SelfTrainingClassifier(estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                                callbacks=None,\n",
       "                                                colsample_bylevel=None,\n",
       "                                                colsample_bynode=None,\n",
       "                                                colsample_bytree=None,\n",
       "                                                device=None,\n",
       "                                                early_stopping_rounds=None,\n",
       "                                                enable_categorical=False,\n",
       "                                                eval_metric=None,\n",
       "                                                feature_types=None, gamma=None,\n",
       "                                                grow_policy=None,\n",
       "                                                importance_type=None,\n",
       "                                                interaction_constraints=None,\n",
       "                                                learning_rate=None, max_bin=None,\n",
       "                                                max_cat_threshold=None,\n",
       "                                                max_cat_to_onehot=None,\n",
       "                                                max_delta_step=None,\n",
       "                                                max_depth=None, max_leaves=None,\n",
       "                                                min_child_weight=None,\n",
       "                                                missing=nan,\n",
       "                                                monotone_constraints=None,\n",
       "                                                multi_strategy=None,\n",
       "                                                n_estimators=None, n_jobs=1,\n",
       "                                                num_parallel_tree=None,\n",
       "                                                random_state=46, ...)),\n",
       " 'classes': [0, 1],\n",
       " 'feature_columns': ['emb_0_struct',\n",
       "  'emb_1_struct',\n",
       "  'emb_2_struct',\n",
       "  'emb_3_struct',\n",
       "  'emb_4_struct',\n",
       "  'emb_5_struct',\n",
       "  'emb_6_struct',\n",
       "  'emb_7_struct',\n",
       "  'emb_8_struct',\n",
       "  'emb_9_struct'],\n",
       " 'feature_importance': None}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [col for col in self.data_df.columns \n",
    "                       if col not in [\"graph_id\", \"label\", \"encoded_label\"]]\n",
    "        \n",
    "y_pred = out['model'].predict(self.data_df[feature_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_file_path = \"../data/ABCDO-full/edges.csv\"\n",
    "node_graph_mapping_file_path = \"../data/ABCDO-full/graph_mapping.csv\"\n",
    "features_file_path = \"../data/ABCDO-full/features.csv\"\n",
    "\n",
    "edges = pd.read_csv(edge_file_path)\n",
    "mapping = pd.read_csv(node_graph_mapping_file_path)\n",
    "_features = pd.read_csv(features_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = _features.sort_values('node_id')['is_outlier'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42857142857142855"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
